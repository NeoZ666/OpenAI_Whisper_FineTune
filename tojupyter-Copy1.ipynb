{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9f14201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset1 = load_dataset(\"TheAIchemist13/hindi_asr_dataset\")\n",
    "dataset2 = load_dataset(\"TheAIchemist13/hindi_asr_dataset_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419c0c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import concatenate_datasets\n",
    "\n",
    "# Assuming you have already loaded the datasets\n",
    "# dataset1 and dataset2 are your loaded DatasetDict objects\n",
    "\n",
    "# Concatenate the individual datasets\n",
    "# combined_train = concatenate_datasets([dataset1[\"train\"], dataset2[\"train\"]])\n",
    "# combined_test = concatenate_datasets([dataset1[\"test\"], dataset2[\"test\"]])\n",
    "\n",
    "# Create a new DatasetDict\n",
    "# combined_dataset = {\"train\": combined_train, \"test\": combined_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14df9ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/datasets/load.py:2508: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the dataset since TheAIchemist13/hindi_asr_dataset couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/TheAIchemist13___hindi_asr_dataset/default/0.0.0/54a3ac1f465f24d9a296bdeb3cd5ee66218f23ae (last modified on Sat Mar 16 20:59:23 2024).\n",
      "Using the latest cached version of the dataset since TheAIchemist13/hindi_asr_dataset couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/TheAIchemist13___hindi_asr_dataset/default/0.0.0/54a3ac1f465f24d9a296bdeb3cd5ee66218f23ae (last modified on Sat Mar 16 20:59:23 2024).\n",
      "Using the latest cached version of the dataset since TheAIchemist13/hindi_asr_dataset_2 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/TheAIchemist13___hindi_asr_dataset_2/default/0.0.0/888cd923658a0396684c0175b19b4e98216e61fe (last modified on Fri Mar 15 19:35:39 2024).\n",
      "Using the latest cached version of the dataset since TheAIchemist13/hindi_asr_dataset_2 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/TheAIchemist13___hindi_asr_dataset_2/default/0.0.0/888cd923658a0396684c0175b19b4e98216e61fe (last modified on Fri Mar 15 19:35:39 2024).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'transcriptions'],\n",
       "        num_rows: 255\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'transcriptions'],\n",
       "        num_rows: 95\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
    "\n",
    "combined_dataset1 = DatasetDict()\n",
    "combined_dataset2 = DatasetDict()\n",
    "combined_dataset = DatasetDict()\n",
    "\"TheAIchemist13/hindi_asr_dataset_2\"\n",
    "\n",
    "combined_dataset1[\"train\"] = load_dataset(\"TheAIchemist13/hindi_asr_dataset\", \"default\", split=\"train\", use_auth_token=True)\n",
    "combined_dataset1[\"test\"] = load_dataset(\"TheAIchemist13/hindi_asr_dataset\", \"default\", split=\"test\", use_auth_token=True)\n",
    "\n",
    "combined_dataset2[\"train\"] = load_dataset(\"TheAIchemist13/hindi_asr_dataset_2\", \"default\", split=\"train\", use_auth_token=True)\n",
    "combined_dataset2[\"test\"] = load_dataset(\"TheAIchemist13/hindi_asr_dataset_2\", \"default\", split=\"test\", use_auth_token=True)\n",
    "\n",
    "combined_dataset[\"train\"] = concatenate_datasets([combined_dataset1[\"train\"], combined_dataset2[\"train\"]])\n",
    "combined_dataset[\"test\"] = concatenate_datasets([combined_dataset1[\"test\"], combined_dataset2[\"test\"]])\n",
    "combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fbd9b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'transcriptions'],\n",
       "        num_rows: 255\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'transcriptions'],\n",
       "        num_rows: 95\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "576aa163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-large-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a541cbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-large-v3\", language=\"Hindi\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a52712d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v3\", language=\"Hindi\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e080ec1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'test'])\n"
     ]
    }
   ],
   "source": [
    "print(combined_dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "089c3518",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "# for split_name in combined_dataset.keys():\n",
    "#     combined_dataset[split_name] = combined_dataset[split_name].cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "combined_dataset = combined_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00eeafba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array \n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids \n",
    "    batch[\"labels\"] = tokenizer(batch[\"transcriptions\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3881b4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': '1.wav', 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, 'transcriptions': 'बीकीपर अक्सर अपनी कॉलोनियों के स्वास्थ्य को सुनिश्चित करने के लिए अमेरिकन फाउलब्रूड परीक्षणों का उपयोग करते हैं।'}\n"
     ]
    }
   ],
   "source": [
    "print(combined_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e54de19",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice = combined_dataset.map(prepare_dataset, remove_columns=combined_dataset.column_names[\"train\"], num_proc=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11fe20d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a505d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a7c422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a8ec477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4398a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4cbc18ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52108feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "010a4cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-large-v3\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=4000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bc18b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.38.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.28.0'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install accelerate -U\n",
    "# !pip install transformers -U\n",
    "import accelerate, transformers\n",
    "\n",
    "print(transformers.__version__)\n",
    "accelerate.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f424ff01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "889803b69e424d5aa53bb69c3d26c0e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79ca25e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "#     tokenizer=tokenizer\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7191d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b07d3ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15d9c53b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'variables' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m variables\n\u001b[1;32m      3\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'variables' is not defined"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "del variables\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a39fda32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 5            |        cudaMalloc retries: 7         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |  12010 MiB |  13718 MiB | 246476 MiB | 234466 MiB |\\n|       from large pool |  12004 MiB |  13709 MiB | 245489 MiB | 233485 MiB |\\n|       from small pool |      6 MiB |      8 MiB |    987 MiB |    980 MiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |  12010 MiB |  13718 MiB | 246476 MiB | 234466 MiB |\\n|       from large pool |  12004 MiB |  13709 MiB | 245489 MiB | 233485 MiB |\\n|       from small pool |      6 MiB |      8 MiB |    987 MiB |    980 MiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |  11880 MiB |  13575 MiB | 246227 MiB | 234347 MiB |\\n|       from large pool |  11873 MiB |  13566 MiB | 245240 MiB | 233367 MiB |\\n|       from small pool |      6 MiB |      8 MiB |    986 MiB |    980 MiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |  14032 MiB |  14066 MiB |  26482 MiB |  12450 MiB |\\n|       from large pool |  14024 MiB |  14056 MiB |  26460 MiB |  12436 MiB |\\n|       from small pool |      8 MiB |     10 MiB |     22 MiB |     14 MiB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |   2021 MiB |   2876 MiB | 193975 MiB | 191954 MiB |\\n|       from large pool |   2019 MiB |   2874 MiB | 192952 MiB | 190932 MiB |\\n|       from small pool |      1 MiB |      2 MiB |   1022 MiB |   1021 MiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |    1341    |    2232    |    8958    |    7617    |\\n|       from large pool |     589    |    1085    |    5572    |    4983    |\\n|       from small pool |     752    |    1147    |    3386    |    2634    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |    1341    |    2232    |    8958    |    7617    |\\n|       from large pool |     589    |    1085    |    5572    |    4983    |\\n|       from small pool |     752    |    1147    |    3386    |    2634    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |     300    |     326    |     394    |      94    |\\n|       from large pool |     296    |     321    |     383    |      87    |\\n|       from small pool |       4    |       5    |      11    |       7    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |     188    |     400    |    4047    |    3859    |\\n|       from large pool |     183    |     317    |    2806    |    2623    |\\n|       from small pool |       5    |      85    |    1241    |    1236    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bcf911aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m      2\u001b[0m     m\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      3\u001b[0m     x \u001b[38;5;241m=\u001b[39m m(x)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "for m in self.children():\n",
    "    m.cuda()\n",
    "    x = m(x)\n",
    "    m.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9995018",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"dataset_tags\": \"mozilla-foundation/common_voice_11_0\",\n",
    "    \"dataset\": \"Common Voice 11.0\",  # a 'pretty' name for the training dataset\n",
    "    \"dataset_args\": \"config: hi, split: test\",\n",
    "    \"language\": \"hi\",\n",
    "    \"model_name\": \"Whisper Small Hi - Sanchit Gandhi\",  # a 'pretty' name for our model\n",
    "    \"finetuned_from\": \"openai/whisper-small\",\n",
    "    \"tasks\": \"automatic-speech-recognition\",\n",
    "    \"tags\": \"hf-asr-leaderboard\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b73a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b473e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "\n",
    "pipe = pipeline(model=\"sanchit-gandhi/whisper-small-hi\")  # change to \"your-username/the-name-you-picked\"\n",
    "\n",
    "def transcribe(audio):\n",
    "    text = pipe(audio)[\"text\"]\n",
    "    return text\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=transcribe, \n",
    "    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"), \n",
    "    outputs=\"text\",\n",
    "    title=\"Whisper Small Hindi\",\n",
    "    description=\"Realtime demo for Hindi speech recognition using a fine-tuned Whisper small model.\",\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c071828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyunpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c976d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -y uninstall zipfile36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7047c778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "\n",
    "# with zipfile.Zipfile(\"\", 'r') as zObject:\n",
    "#     zObject.extractall(path=\"./darpg_audiofiles\")\n",
    "    \n",
    "# from pyunpack import Archive\n",
    "\n",
    "# Archive('./darpg_audiofiles.zip').extractall('./')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c18435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=False, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "generate_kwargs = {\"language\": \"hi\"}\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=5,\n",
    "    batch_size=16,\n",
    "    return_timestamps=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    "    generate_kwargs=generate_kwargs\n",
    ")\n",
    "\n",
    "result = pipe(\"data_files/_6002335182.mp3\")\n",
    "print(result[\"text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "your_env_name",
   "language": "python",
   "name": "your_env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
